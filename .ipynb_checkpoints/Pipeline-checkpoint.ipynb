{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1462887e-0bb7-48b5-acf7-f0f8b5e0c556",
   "metadata": {},
   "source": [
    "# ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b42fb-06b0-4804-83ee-195b9bc304f0",
   "metadata": {},
   "source": [
    "This application fetches real estate listings from a webpage, tidies up the data, and stores it in an Amazon S3 bucket. \n",
    "\n",
    "It uses Python libraries like requests and BeautifulSoup for web scraping, and Boto3 for interacting with S3. \n",
    "\n",
    "Additionally, it's orchestrated by Apache Airflow, which schedules and manages the entire ETL process.\n",
    "\n",
    "This setup automates the extraction, transformation, and loading of real estate data, making it easy to keep the listings up-to-date and accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14493e-6e7a-48ef-8730-9595ac5dd7d7",
   "metadata": {},
   "source": [
    "## Summary of the ETL Process Application:\r\n",
    "\r\n",
    "### Data Retrieval:\r\n",
    "- Utilizes requests library to send HTTP requests and BeautifulSoup library to parse HTML content of a webpage.\r\n",
    "- Scrapes specific data from the webpage and saves it to a pandas dataframe.\r\n",
    "\r\n",
    "### Data Preprocessing and Cleaning:\r\n",
    "- Defines functions to extract specific details from the dataframe and clean the data.\r\n",
    "- Extracts details like area, number of bedrooms, suites, parking spaces, etc.\r\n",
    "- Cleans and preprocesses price data.\r\n",
    "\r\n",
    "### Loading Data to Amazon S3:\r\n",
    "- Loads AWS configuration from a JSON file.\r\n",
    "- Uploads a CSV string to an S3 bucket using Boto3 S3 client.\r\n",
    "\r\n",
    "### DAG and Task Dependencies:\r\n",
    "- Sets up a Directed Acyclic Graph (DAG) named \"automated_etl_process\" using Airflow.\r\n",
    "- DAG is scheduled to run daily.\r\n",
    "- Defines three tasks for data extraction, transformation, and loading, represented by PythonOperator.\r\n",
    "- Establishes task dependencies where the extraction task precedes transformation, and transformation precedes loading.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6624bb0-be17-400d-aa12-1656f92f6024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a6571-8684-4f50-9534-17b85e86793c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33baa6a2-e75a-4d31-bfd4-bce385e57f37",
   "metadata": {},
   "source": [
    "## 1. Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8dccc-1cf8-49f2-b0c3-9caca3e5db38",
   "metadata": {},
   "source": [
    "The following script is used to retrieve data from a webpage. It utilizes the requests library to send HTTP requests and the BeautifulSoup library to parse the HTML content of the webpage. Only the data of interest was saved to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2840279-897a-4b5e-9592-44f96078796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d34f81-0ab1-49cc-8c43-0d7325d62249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    page = 1\n",
    "    data = []\n",
    "\n",
    "    while True:\n",
    "        full_url = f'{url}?pagina={page}'\n",
    "        response = requests.get(full_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            cards = soup.find_all('a', class_='new-card')\n",
    "\n",
    "            if not cards:\n",
    "                print(\"No more pages found or no cards on the page.\")\n",
    "                break\n",
    "\n",
    "            for card in cards:\n",
    "                title = card.find('h2', class_='new-title phrase').get_text(strip=True) if card.find('h2', class_='new-title phrase') else None\n",
    "                description = card.find('h3', class_='new-desc phrase').get_text(strip=True) if card.find('h3', class_='new-desc phrase') else None\n",
    "                simple_description = card.find('h3', class_='new-simple phrase').get_text(strip=True) if card.find('h3', class_='new-simple phrase') else None\n",
    "                price = card.find('div', class_='new-price').get_text(strip=True) if card.find('div', class_='new-price') else None\n",
    "                details = [li.get_text(strip=True) for li in card.find_all('li')] if card.find_all('li') else []\n",
    "                additional_info = card.find('div', class_='new-text phrase').get_text(strip=True) if card.find('div', class_='new-text phrase') else None\n",
    "                agency = card.find('img', alt=True)['alt'] if card.find('img', alt=True) else None\n",
    "                creci = card.find('div', class_='creci').get_text(strip=True) if card.find('div', class_='creci') else None\n",
    "\n",
    "                data.append([title, description, simple_description, price, details, additional_info, agency, creci])\n",
    "\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage. Status Code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606ec99-56d3-4b78-911d-7dcff2ceb38d",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e3799-7467-41b3-9994-8f653ddccfc2",
   "metadata": {},
   "source": [
    "This code defines the transform_data function, on which we have the functions extract_details(), extract_price() and extract_numbers.  to extract specific details and do some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae496d-6bae-4587-9b4e-fc59d7ee40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39846e85-3b81-4b2a-b5c0-4e7b5e9e2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    def extract_details(details):\n",
    "        area = None\n",
    "        quartos = None\n",
    "        suites = None\n",
    "        vagas = None\n",
    "        outros = None\n",
    "\n",
    "        for detail in details:\n",
    "            if 'm²' in detail:\n",
    "                area = detail\n",
    "            elif 'quarto' in detail.lower():\n",
    "                quartos = detail\n",
    "            elif 'suíte' in detail.lower():\n",
    "                suites = detail\n",
    "            elif 'vaga' in detail.lower():\n",
    "                vagas = detail\n",
    "            else:\n",
    "                outros = detail\n",
    "\n",
    "        return area, quartos, suites, vagas, outros\n",
    "\n",
    "    df['area'], df['quartos'], df['suites'], df['vagas'], df['outros'] = zip(*df['Details'].apply(extract_details))\n",
    "\n",
    "    df.fillna('na', inplace=True)\n",
    "\n",
    "    df.drop(columns=['Details'], inplace=True)\n",
    "\n",
    "    def extract_price(price_text):\n",
    "        price = 'na'\n",
    "        price_per_m2 = 'na'\n",
    "\n",
    "        # Check if 'R$' appears twice in the text\n",
    "        if price_text.count('R$') == 2:\n",
    "            prices = re.findall(r'R\\$(.*?)R\\$(.*?)$', price_text)\n",
    "            price = 'R$' + prices[0][0].strip()\n",
    "            price_per_m2 = 'R$' + prices[0][1].strip()\n",
    "        else:\n",
    "            price = price_text\n",
    "\n",
    "        return price, price_per_m2\n",
    "\n",
    "    df['price'], df['price_per_m2'] = zip(*df['Price'].apply(extract_price))\n",
    "\n",
    "    df.drop(columns=['Price'], inplace=True)\n",
    "\n",
    "    def extract_numbers(text):\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', text)\n",
    "        return ''.join(numbers)\n",
    "\n",
    "    df['price'] = df['price'].apply(lambda x: extract_numbers(x.replace('R$', '').replace('Valor', '').replace('m²', '')))\n",
    "    df['price_per_m2'] = df['price_per_m2'].apply(lambda x: extract_numbers(x.replace('R$', '').replace('Valor', '').replace('m²', '')))\n",
    "\n",
    "    df['price'] = df['price'].replace('', 'na')\n",
    "    df['price_per_m2'] = df['price_per_m2'].replace('', 'na')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc48e3-e94d-495d-9e8a-209aba96f6a9",
   "metadata": {},
   "source": [
    "## 3. Loading Data to Amazon S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2647cc2-6546-42a5-b8c6-1e2f9dd0095c",
   "metadata": {},
   "source": [
    "Now we load AWS configuration from a JSON file named `config.json`, then uses it to upload a CSV string to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503eeaf-a7e9-47c7-9e33-8ea9e8977f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    #AWS configuration\n",
    "    with open('config.json') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    #Boto3 S3 client\n",
    "    s3_client = boto3.client('s3', \n",
    "                             aws_access_key_id=config['aws_access_key_id'],\n",
    "                             aws_secret_access_key=config['aws_secret_access_key'],\n",
    "                             region_name=config['region_name'])\n",
    "\n",
    "    # Define the target S3 bucket and key (file name)\n",
    "    bucket_name = config['bucket_name']\n",
    "    key = 'target_destination.csv'\n",
    "\n",
    "    #convert DataFrame \n",
    "    csv_buffer = df.to_csv(index=False)\n",
    "\n",
    "    #Upload\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=key, Body=csv_buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa04c3-700d-415e-a37f-f0be2b9e4bb0",
   "metadata": {},
   "source": [
    "## 3.DAG and Task Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e03a6-c92b-4301-97b4-25c15ca312b2",
   "metadata": {},
   "source": [
    "This code sets up a DAG named \"automated_etl_process\" with default arguments specifying the owner, start date, retries, and retry delay. It schedules the DAG to run daily. Three tasks are defined for data extraction, transformation, and loading, each represented by a PythonOperator. Task dependencies are established where the data extraction task precedes the transformation task, and the transformation task precedes the loading task.\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddec498-6e32-45ba-ae74-e6367f4e6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Define default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2022, 3, 7),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "#Define the DAG\n",
    "dag = DAG(\n",
    "    'automated_etl_process',\n",
    "    default_args=default_args,\n",
    "    description='Automated ETL Process',\n",
    "    schedule_interval='@daily',  # Run the DAG daily\n",
    ")\n",
    "\n",
    "#Define Tasks\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_data',\n",
    "    python_callable=transform_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    python_callable=load_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "#Define task dependencies\n",
    "extract_task >> transform_task >> load_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b1043-f90d-49d6-b985-a1803f83aacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
